In Sept 2021, here was a list of questions/answers for a data science job:
---------------------------
Describe a specific situation in which you had to apply strong knowledge of Python/SQL. I have one thousand hours training with SQL data manipulation, but mostly using Python. For me personally, I find it easier to use SQL to extract the database 
and then use Python to manipulate data sets. For example, at Illinois State University, I invented a Python data science pandas dataFrame automation program that compares 
thousands of data transactions to Access (SQL) database records which detects unbalanced accounts, new transactions, and human error. When Dr. Walstrom the director of Python for 
Data Science at Illinois State University saw my project, he indirectly told me to apply for this job. See code for reference: https://github.com/ScottFrederickSchmidt/AccountingAutomation

On my "Amazon" site, I built an entire SQL database that was on the backend. It is connected on the frontend using PHP. The basic SQL terminology were all used for this project
(i.e. select, insert, update, delete). In addition, it has a basic SQL search and recommendation section. I am currently improving SQL data manipulation because 
it is important when datasets get too complex or big. For example, Excel has a max of around two million rows while Python can have a max out on memory with very large, complex dataFrame. 
See code for reference: https://github.com/ScottFrederickSchmidt/Amazon-like-site

I am currently taking two SQL courses, one at Illinois State University and one on Udemy.
Pre-processing on a database level has better performance. I am also using Leetcode to improve my SQL programming skills. 
See code for reference: https://github.com/ScottFrederickSchmidt/Complete-SQL-Bootcamp-and-SQL-DataAnalytics-Course

Implementing TABLEAU and SQL is used to connect, calculate, and visualize to can create graphs, charts, reports, dashboards. 
I briefly used Tablaeu at Clark Street Capital and have been taking a class to get more experience in it.
This is an easy solution to be able to visualize large data sets.


----------------------
Please detail where you gained knowledge of statistics and machine learning.

Using portfolio123.com at Douglas Capital Management is machine learning, by using a database of complex finance formulas to predict future returns. 
We had simple MATLAB functions setup to help determine the statistics of the best results. One would have to consider missing data and overfitting for using
a projected return model.

During my MBA program as an ISU graduate assistant, I generated a 95% r-squared using data analytics (depends on adjustment to overfitting). 
This identified the best candidates that would most likely successfully graduate at ISU. This was the final MBA capstone course, MQM497. 

During my MBA program at Illinois State University, I gained a vast amount of knowledge with using statics with business practices. 
Statistics was used all the time in my Data Science bootcamp.  The regression line I am most familiar with is Sector Vector Machine (SVM) because that is what bigger investment
firms funds use. It was also briefly discussed in my Python Data Science course which can be found in the below link:
https://github.com/ScottFrederickSchmidt/Python-DataScience-Bootcamp

At Clark Street Capital, I used machine learning and correlations for loan and risk credit score. 
My "Amazon" website has a basic recommendation system that makes suggestions based upon current purchases.

My Python accounting automation projects at Illinois State University could be closely related to "machine learning" but not to an exact fit.
For example, I created a Python pandas script that can read data to update a monthly budget report with new information.
The other project can predict unbalanced accounts without the need of a human. 
https://github.com/ScottFrederickSchmidt/AccountingAutomation

------------------------
Please describe a complex predictive model that you have developed

At Douglas Capital Management, I used portfolio123.com database with hundreds of finance formulas to predict future returns. For instance,
 OperCashFlTTM - CapExTTM + IntExpTTM*(1-TaxRate%TTMInd/100) would be inserted into a database with an optimization process to see how this one 
 formula can affect future returns. We also had to account for missing and/or overfitting data. Basic MATLAB functions would be used to help find the statistic relevance of each formula. 
In 2019 of Q4, our returns for our aggressive trading returns were in the top ten. However, I believe in efficient market theory and that more risk equals greater returns, 
so I have left the investment industry. I admit I am not smarter than the stock market on a simple buy and hold strategy on the USA SP500.

During my MBA program as an ISU graduate assistant, I generated a 95% r-squared using data analytics (depends on adjustment to overfitting). 
This identified the best candidates that would most likely successfully graduate at ISU. This was the final MBA capstone course, MQM497. 
In conclusion, Pell Grants and amount of initiated contacts are the two biggest factors in whether a student enrolled at the university or not. 
These two factors alone can make a fairly good prediction on whether a student enrolls at the university or not. 
Neural Network  According to Chaughan, “Neural networks are non-linear statistical data modeling tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.
Using neural networks as a tool, data warehousing firms are harvesting information from datasets in the process known as data mining.” 
Neural networks help the user make more informed decisions. Our R-Squared was .99 which is shows a very strong correlation; however, this might be extremely high because of overfitting. 
In conclusion, neural networks show that undecided majors, Pell Grant, distance from ISU, # of initiated events, family income median, total consumer expenditures, high school low income percentage, and high school GPA are the major factors in whether a student enrolls at Illinois State University or not. 
Confirmatory Analysis: The purpose of a confirmatory analysis is to test whether the data fit a hypothesized measurement model. 
In other words, it tests whether measures of a factor are consistent with a researcher's understanding of a factor. 
This test confirms that ACT, GPA, major, Pell Grant, and # of initiated events make a significant difference in students enrolling at Illinois State University.  

At Clark Street Capital, I used basic machine learning and correlations to predict loan risk. 
My "Amazon" website has a basic recommendation system that would predict what purchases the customer might want to buy next.

------------------------
What are the tools you have most commonly used to complete predictive analysis? 

At Illinois State University, we used JMP ® Pro Predictive to predict the best students to graduate at Illinois State University. 

For Python, I have used Pandasm Numpy, seaborn, sklearn, and matplotlib.pyplot as imports.
I mostly used pandas dataFrames at Clark Street Capital and Douglas Capital Management.

At Clark Street Capital, I briefly used Tablaeu. This is what I still use today and am taking a more in depth class to see its full potential.
I have briefly used Power BI and have purchased a Udemy course on it. I plan on finishing that course during my 2 weeks off over winter break!





