Hi Kevin. Glad we get to chat since being ISU MBA graduates together. 

1. How does this position fit with the career path you envision for yourself?
I believe Python is a superpower, and I want to help people by making data driven decisions.
The ISU data science team will able me to grow as a programmer and learn from my peers as I continue to use the 
Python data science skills I have gained from prestigious hedge funds to benefit the education system.
Illinois State University is home to me as an alum, employee, former graduate assistant, current student, and future student,
and I believe the data science team will give me the opportunity to apply my knowledge as someone who knows the university culture. 



2. Data extrapolation and analysis are critical to accurate models being developed.  Please describe an example of data extract and analysis that you have completed.
At Illinois State University, I invented a Python data science pandas dataFrame automation program that 
compares thousands of data transactions to database records which detects unbalanced accounts, new transactions, and human error: 
https://github.com/ScottFrederickSchmidt/AccountingAutomation
This was a important task because it involves accounting. If the program is not 100% accurate, one could
get involved in an audit that could cost the company thousands of dollars and a bad reputation. In accounting, one must be exact.
There are no guesses. Accuracy, attention to detail and perfection is of highest importance.
My accounting software can predict if an account is not balanced with 100% accurary. 
If my accounting software was implemented on a global basis, I think it would save ISU over one hundred thousand dollars a year.
It would also not automate jobs but instead make the job of an accountant more enjoyable. 

Likewise, at Douglas Capital Management I programmed Python financial automation using pandas, ib_insync, and operating systems (os). 
One slight mistake with the trading bot could mean a loss of money if the bot gathered incorrect information using the ib_insync package.
I also used Python to webpage data scrapping using BeautifulSoup (bs4), requests, selenium, webdriver, and to_csv. For example, I developed 
a weather bot to help get the average temperature across major cities in the United States to help determine the fair value of the 
price for natural gas depending on the weather during winter. I can probably show my Python weather bot I created on request.

At Clark Street Capital, we used pandas, numpy, and sklearn to predict loan credit risk. Credit risk was predictive through variables such 
as employment, zip code and credit score. This risk was important to analyze because loans were bundled together by their risk score.

Coding efficiency, queries, and data manipulation is my strength as I have over one thousand hours in Python data manipulation 
which can be viewed on both GitHub and YouTube. One must not just code, but code in the correct order and logic.
My data manipulation skills can be viewed here:
https://github.com/ScottFrederickSchmidt/Project-Euler-Python
https://github.com/ScottFrederickSchmidt/LeetCode

For instance, my first solution to problem 44 took 86 seconds. 
After putting five hours worth of changing programming logic, I was able to get the code to run in under two seconds 
with a new, revised solution.



3. Please describe the experience you have creating predictive models.

At Douglas Capital Management, I used portfolio123.com database with hundreds of finance formulas to predict future returns. For instance,
OperCashFlTTM - CapExTTM + IntExpTTM*(1-TaxRate%TTMInd/100) would be inserted into a database with an optimization process to see how this one 
formula can affect future returns. More important formulas such as annual growth rate would have a higher significance for returns.
We also had to account for missing data. On occasion, missing data was completely deleted. But the majority of the time,
missing data was filled in using a estimate by a regression line. Basic MATLAB functions would be used to help find the statistic relevance of each formula. 
In 2019, our returns were in the top ten. During this time, I also designed Python financial automation using pandas and os that would teach
an AI bot when to make a trade. 

During my MBA program as an ISU graduate assistant, I generated a 95% r-squared using data analytics (depends on adjustment to overfitting). 
This identified the best candidates that would most likely successfully graduate at ISU. This was the final MBA capstone course, MQM497. 
In conclusion, Pell Grants and amount of initiated contacts are the two biggest factors in whether a student enrolled at the university or not. 
These two factors alone can make a fairly good prediction on whether a student enrolls at the university or not. 
In conclusion, neural networks show that undecided majors, Pell Grant, distance from ISU, # of initiated events, family income median, total consumer expenditures, 
high school low income percentage, and high school GPA are the major factors in whether a student enrolls at Illinois State University or not. 
The test confirms that ACT, GPA, major, Pell Grant, and # of initiated events make a significant difference in students enrolling at Illinois State University.  

At Clark Street Capital, I used machine learning and correlations to predict loan risk using customer and loan data. 
My "Amazon" website has a basic recommendation system that would predict what purchases the customer might want to buy next.

Sample predictive Python projects using numpy, pandas, matplotlib.plplot, and sklearn.model_selection can be found my GitHub below:
https://github.com/ScottFrederickSchmidt/PredictivePythonModeling


4. In terms of SQL syntax or query structure, what are some things you focus on when writing a query to ensure it functions as efficiently as possible?

The larger the table, the longer it takes to read the data. Using a JOIN statement will increase the runtime. 
One should try to only use the columns needed. 
One should never use the * operator. Including specific column names enhances performance.
It also makes the code more reusable in the future (if more columns get added later).
Using the * function is also a security issue. One will accidenty gather confidential information such as
addresses, studentID, age, birthday, and confidential information into an csv file when it should not be there.
One should not repeat code. In my automation project, I made sure to never use a line twice.

Coding efficiency, queries, and data manipulation is my strength as I have over one thousand hours in Python data manipulation 
which can be viewed on both GitHub and YouTube.One must not just code, but code in the correct order and logic.
My data manipulation skills can be viewed on my GitHub below:
https://github.com/ScottFrederickSchmidt/Project-Euler-Python
https://github.com/ScottFrederickSchmidt/LeetCode


Memory is an important issue of the life of a programmer. This is because Excel has a max of around two million rows 
while Python can have a max out on memory with very large, complex dataFrame. However, having rows at this magnitude does not happen often. 
Therefore, Python cam be used as primary data manipulation language using pandas.
This is what Python was specifically designed for and why it is now one of the most used languages in the world.
In a rare event there was data with more than ten million rows, I would be able to manipulate the data with SQL but it would
take me much longer than with Python. Challenging SQL analytics was done in my Udemy course which can be found below:
https://github.com/ScottFrederickSchmidt/Complete-SQL-Bootcamp-and-SQL-DataAnalytics-Course


On my "Amazon" site, I built an entire SQL database that was on the backend. It is connected on the frontend using PHP. 
The basic SQL terminology were all used for this project (i.e. select, insert, update, delete). 
In addition, it has a basic SQL search and recommendation section. All these SQL queries happened with high performance speed.
See code for reference: https://github.com/ScottFrederickSchmidt/Amazon-like-site




5. Part of the Data Scientist role will be to provide visualizations that will be able to explain the analysis that you have performed. 
What tools have you used to create data visualizations?  

I have mostly used Tableau and Excel for visualizations at Clark Street Capital. In a recent data science
poll on LinkedIn I saw Excel was ranked as being "highly underrated" for data science. 

Tableau gives a data scientist an easy way to explain data to an individual who may not have a statistics or coding background. 
In fact, Tableau makes it easy for even developers to understand data and make analytical decisions. 

Because I have a web development background, I could also display graphs on the Illinois State University website if needed with JavaScript such
as https://www.chartjs.org/docs. However, I prefer not to program in JavaScript/webdevelopment but could if needed. 


6. Please describe a presentation that you have completed to present the results of a predictive model you developed.	
At Clark Street Capital, I had to help package and sell at $42 million dollar non-performing commercial real estate loan bundled together 
as a mortgage-back security through a major United States bank. Based upon our predictive analysis, the goal was to sell the loan
around eighty cents on the dollar. I had to help present this MBS product over phone individually to around a hundred banking clients across
the United States.


At Illinois State University, I invented a Python data science pandas dataFrame automation program that 
compares thousands of data transactions to database records which detects unbalanced accounts, new transactions, and human error: 
https://github.com/ScottFrederickSchmidt/AccountingAutomation


I have presented this project to several programming professors and directors because I have wanted to try to implement my 
program on a more frequent and global basis. I truly believe my program can save ISU over a hundred thousand dollars a year if correctly implemented. 
It will also make the life of an accountant more enjoyable. Presenting programming code is more of a challenge than I originally thought because 
I have concluded that data manipulation and programming are almost two completely different specialized subjects. 


Sample predictive Python projects using numpy, pandas, matplotlib.plplot, and sklearn.model_selection can be found my GitHub below:
https://github.com/ScottFrederickSchmidt/PredictivePythonModeling


7. Have you ever had to “sell” an idea to your co-workers, supervisor or customers? How did you do it?  
Great question! I think my data science accounting automation software I have developed would save Illinois State University, 
if implemented across the entire university, well over a hundred thousand dollars a year. But I have not had the confidence to "sell" my idea.

A prepared prototype using fake data was built before hand so I had confidence the program would work.
When I did invent my automation program, I did request permission to use the data first.
I also made sure that no confidential information would accidently be leaked.
The idea was successfully implemented with highest honors. 


8. During analysis, how do you treat missing values?
The simple, easy way to treat missing values is to simply delete any data row that has a missing value using a simple Python script such as
df.dropna(). This is my preferred method. This method does not make assumptions on top of assumptions.

Another way is to find the mean of the data and assume that the missing value is that number. 
A bad choice would be to fill the missing data with a 0 because this would easily distort the data.

Lastly, if there is not a lot of data points with lots of missing data, one should consider not using that data at all. 
A lack of data points will not create an accurate prediction. 

After writing out my answer, I realized that on July 2nd, 2020, I wrote a brief article on my GitHub on this exact question
which can be viewed here:
https://github.com/ScottFrederickSchmidt/PythonDataScience/blob/master/dropna()%20or%20fillna()



9. What are the tools you have most commonly used to complete predictive analysis?  Which one is your favorite?   
I like using using Python because it has a wide variety of tools such as pandas, numpy, and sklearn. 
This is what I used to automate the accounting analytics predictions at Illinois State University,
financial automation at Douglas Capital Management, and predictive analytics at Clark Street Capital. 
This is what was taught in my Data Science Bootcamp on Udemy and by my data science mentor, Sukumar Lagapati.
